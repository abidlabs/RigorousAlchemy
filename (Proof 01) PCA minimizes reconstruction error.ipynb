{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: \n",
    "\n",
    "* Principal Component Analysis (PCA) is a ubiquitous tool for reducing the dimensionality of data. \n",
    "* The method is used to take data that is high dimensional and project it down to a lower dimension, usually 2 or 3, so that it can be visualized. \n",
    "* It is a classic example of unsupervised learning, as it allows one to discover patterns within the data.\n",
    "\n",
    "---------------\n",
    "\n",
    "**What We're Proving**: There may be many ways to \"project\" data $X$ from high dimensions to lower dimensions $\\hat{X}$. PCA claims to finds the _best_ linear projection i.e. the one that minimizes the reconstruction error $\\lvert\\lvert X - \\hat{X}\\rvert\\rvert$ (the norm is the Frobenius norm, calculated by summing the square of each entry in that matrix). It does so by computing the covariance matrix of the data $C \\equiv X^\\top X$, and then projecting the data on to the first few eigenvectors of $C$. \n",
    "\n",
    "Why does this method minimize the recontruction error? Here we prove that it does. For simplicity, we will consider the case of PCA from $d$ dimensions to $1$ dimension. Without loss of generality, we will also assume that $X$ is \"centered,\" meaning that the mean across all samples of every data dimension is 0.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "\n",
    "**Step 1: (How to write a projection mathematically)** We start by noting that a projection of $X$ from $d$ dimensions to 1 dimension can always be written as\n",
    "\n",
    "$$ X u u^\\top $$\n",
    "\n",
    "for a suitable unit vector $u$ that points in the direction of the projection. (To confirm this, note that every data point can be decomposed into a component in the direction of $u^\\top$ and a component perpendicular to $u$, and $uu^\\top$ preserves that component in the direction of $u^\\top$.).\n",
    "\n",
    "Thus, we can write the problem of minimizing the reconstruction loss as a minimization over unit vectors (again the norm here is the Frobenius norm):\n",
    "\n",
    "$$ \\arg\\min_u \\, \\lvert\\lvert X - Xuu^\\top \\rvert\\rvert$$\n",
    "\n",
    "**Step 2: (From minimization to maximization)**  Note that this the same as:\n",
    "\n",
    "$$ \\arg\\max_u \\, \\lvert\\lvert Xuu^\\top \\rvert\\rvert$$\n",
    "\n",
    "Why? Consider any data point $x_i$ (the $i^\\text{th}$ row of $X$). Note that by the Pythagorean theorem, $\\lVert x_i \\rVert^2 = \\lVert x_i - x_i uu^\\top \\rVert^2 + \\lVert x_i uu^\\top \\rVert^2 $ (since the terms on the right are projections onto orthogonal subpsaces). Thus, to maximize one means to minimizes the other. This holds true for every data point individually, and thus holds true for the entire matrix $X$.\n",
    "\n",
    "**Step 3: (Rewriting the Frobenius norm as a trace)** Using very common trick in linear algebra, we rewrite the Frobenius norm of a matrix $A$ as a trace: $\\text{tr}(AA^\\top)$.\n",
    "\n",
    "$$ \\arg\\max_u \\, \\text{tr} ((Xuu^\\top)^\\top Xuu^\\top) $$\n",
    "$$ = \\arg\\max_u \\, \\text{tr} (uu^\\top X^\\top Xuu^\\top) $$\n",
    "\n",
    "**Step 4: (Cycling through the trace)** Since the trace is invariant to cyclic permutations, we can play a trick taking advantage of the fact that $u$ is a unit vector:\n",
    "\n",
    "$$ \\arg\\max_u \\, \\text{tr} (u^\\top X^\\top Xuu^\\top u) $$\n",
    "\n",
    "$$ = \\arg\\max_u \\, \\text{tr} (u^\\top X^\\top Xu) $$\n",
    "\n",
    "$$ = \\arg\\max_u \\, \\text{tr} (u^\\top C u) $$\n",
    "\n",
    "$$ = \\arg\\max_u \\, u^\\top C u $$\n",
    "\n",
    "**Step 5: (From maximization to eigenvectors)** Now note that $C$ is a symmetric matrix. What this means is that it has a full set of orthornormal eigenvectors. So let's write $u$ out in the basis of of the eigenvectors of $C$, call them $e_1, \\ldots e_d$ with respective eigenvalues $\\lambda_1 \\ldots \\lambda_d$.\n",
    "\n",
    "Then, \n",
    "\n",
    "$$ \\arg\\max_u \\, u^\\top C u $$\n",
    "\n",
    "$$ = \\arg\\max_{\\alpha_i} \\, (\\alpha_1 e_1 + \\ldots \\alpha_d e_d)^\\top C (\\alpha_1 e_1 + \\ldots \\alpha_d e_d) $$\n",
    "\n",
    "$$ = \\arg\\max_{\\alpha_i} \\, (\\alpha_1 e_1 + \\ldots \\alpha_d e_d)^\\top (\\lambda_1 \\alpha_1 e_1 + \\ldots \\lambda_d \\alpha_d e_d) $$\n",
    "\n",
    "$$ = \\arg\\max_{\\alpha_i} \\, (\\lambda_1 \\alpha_1^2 + \\ldots \\lambda_d \\alpha_d^2) $$\n",
    "\n",
    "where the final equality follows from the fact that the eigenvectors form an orthornormal set. Now because $\\alpha_1^2 + \\ldots \\alpha_d^2 = 1$ (remember, we these are the coefficients we get when we decompose a unit vector along a set of orthonormal vectors), we see we are simply taking a weighted average of the eigenvalues of $C$. WLOG, assume that $\\lambda_1$ is the largest eigenvalues. We should clearly put all of the \"weight\" to $\\alpha_1$. \n",
    "\n",
    "This means that the best projection $u$ to use is the eigenvector $e_1$ with the largest eigenvalue. This is the projection that minimizes our reconstruction error!\n",
    "\n",
    "--------------\n",
    "QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"viz/visualization01b.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML, Image\n",
    "Image(url='viz/visualization01b.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What if instead of the Frobenius norm, we minimized a different norm? One can imagine a norm that is more robust to outliers for example. For which norms can we derive a closed-form solution?\n",
    "1. On a similar note, we could include an L1 regularization term that encourages sparsity along the projections. i.e. so that the projections only includes a few dimensions. Is this still solvable? I believe this is known as Sparse PCA.\n",
    "1. Is there any significance ot the sinusoidal shape of the reconstruction error and covariance curves? What is there exact form in terms of the rotation angle? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
